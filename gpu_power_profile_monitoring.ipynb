{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "GPU Power Monitoring - With GPU State Metadata Tracking\n",
        "For Google Colab - monitors power with detailed state information\n",
        "\n",
        "NEW FEATURES:\n",
        "- Training state tracking: initiation, forward_pass, backward_pass, communication\n",
        "- Inference state tracking: processing_batch_N, waiting_for_queries\n",
        "- Millisecond-scale sampling (10ms-1000ms)\n",
        "- Gamma distribution for realistic query arrivals\n",
        "- Detailed CSV output with state metadata\n",
        "\n",
        "IMPORTANT: Run this in a FRESH Colab session or restart the kernel first!\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import threading\n",
        "from collections import defaultdict, deque\n",
        "import numpy as np\n",
        "import gc\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# ============================================================================\n",
        "# MEMORY CLEANUP\n",
        "# ============================================================================\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Aggressively clean up GPU memory\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "# ============================================================================\n",
        "# GPU MONITORING CLASS WITH STATE TRACKING\n",
        "# ============================================================================\n",
        "\n",
        "class GPUMonitorWithStates:\n",
        "    \"\"\"Monitor GPU metrics with state metadata tracking\"\"\"\n",
        "\n",
        "    def __init__(self, gpu_id=0, sampling_interval_ms=100):\n",
        "        \"\"\"\n",
        "        Initialize GPU monitor with state tracking.\n",
        "        \n",
        "        Args:\n",
        "            gpu_id: GPU device ID to monitor\n",
        "            sampling_interval_ms: Sampling interval in milliseconds\n",
        "        \"\"\"\n",
        "        self.gpu_id = gpu_id\n",
        "        self.sampling_interval_ms = max(10, min(1000, sampling_interval_ms))\n",
        "        self.monitoring = False\n",
        "        self.metrics = defaultdict(list)\n",
        "        self.timestamps = []\n",
        "        self.gpu_states = []  # NEW: Track GPU state at each sample\n",
        "        self.monitor_thread = None\n",
        "        self.error_count = 0\n",
        "        self.max_errors = 10\n",
        "        self.sample_count = 0\n",
        "        self.dropped_samples = 0\n",
        "        \n",
        "        # State tracking\n",
        "        self.current_state = \"idle\"\n",
        "        self.state_lock = threading.Lock()\n",
        "        \n",
        "        # Performance tracking\n",
        "        self.query_times = deque(maxlen=100)\n",
        "        self.adaptive_sampling = True\n",
        "        \n",
        "        print(f\"\\n🔧 GPU Monitor initialized with {sampling_interval_ms}ms sampling interval\")\n",
        "        print(f\"   Expected samples per second: {1000/sampling_interval_ms:.1f}\")\n",
        "        print(f\"   State tracking: ENABLED\")\n",
        "\n",
        "    def set_state(self, state):\n",
        "        \"\"\"Thread-safe state setter\"\"\"\n",
        "        with self.state_lock:\n",
        "            self.current_state = state\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"Thread-safe state getter\"\"\"\n",
        "        with self.state_lock:\n",
        "            return self.current_state\n",
        "\n",
        "    def get_gpu_info(self):\n",
        "        \"\"\"Get GPU brand and model information\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                ['nvidia-smi', '--query-gpu=name,driver_version', '--format=csv,noheader'],\n",
        "                capture_output=True, text=True, check=True, timeout=2\n",
        "            )\n",
        "            gpu_name, driver_version = result.stdout.strip().split(', ')\n",
        "            return gpu_name, driver_version\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Error getting GPU info: {e}\")\n",
        "            return \"Unknown\", \"Unknown\"\n",
        "\n",
        "    def get_gpu_metrics(self):\n",
        "        \"\"\"Query GPU metrics - optimized for millisecond sampling\"\"\"\n",
        "        query_start = time.perf_counter()\n",
        "        \n",
        "        try:\n",
        "            query = [\n",
        "                'nvidia-smi',\n",
        "                '--query-gpu=power.draw,temperature.gpu,utilization.gpu,memory.used,memory.total',\n",
        "                '--format=csv,noheader,nounits',\n",
        "                '-i', str(self.gpu_id)\n",
        "            ]\n",
        "\n",
        "            timeout = max(0.05, self.sampling_interval_ms / 1000 * 0.5)\n",
        "            result = subprocess.run(query, capture_output=True, text=True, check=True, timeout=timeout)\n",
        "            metrics_str = result.stdout.strip()\n",
        "\n",
        "            parts = [p.strip() for p in metrics_str.split(',')]\n",
        "\n",
        "            metrics = {\n",
        "                'power_draw_w': float(parts[0]) if parts[0] not in ['[N/A]', 'N/A'] else None,\n",
        "                'temperature_c': float(parts[1]) if parts[1] not in ['[N/A]', 'N/A'] else None,\n",
        "                'utilization_pct': float(parts[2]) if parts[2] not in ['[N/A]', 'N/A'] else None,\n",
        "                'memory_used_mb': float(parts[3]) if parts[3] not in ['[N/A]', 'N/A'] else None,\n",
        "                'memory_total_mb': float(parts[4]) if parts[4] not in ['[N/A]', 'N/A'] else None,\n",
        "            }\n",
        "\n",
        "            # Try to get voltage and current\n",
        "            try:\n",
        "                voltage_query = ['nvidia-smi', '--query-gpu=voltage,current', '--format=csv,noheader,nounits', '-i', str(self.gpu_id)]\n",
        "                voltage_result = subprocess.run(voltage_query, capture_output=True, text=True, timeout=timeout)\n",
        "                if voltage_result.returncode == 0:\n",
        "                    voltage_parts = [p.strip() for p in voltage_result.stdout.strip().split(',')]\n",
        "                    metrics['voltage_v'] = float(voltage_parts[0]) if voltage_parts[0] not in ['[N/A]', 'N/A', '[Not Supported]'] else None\n",
        "                    metrics['current_a'] = float(voltage_parts[1]) if len(voltage_parts) > 1 and voltage_parts[1] not in ['[N/A]', 'N/A', '[Not Supported]'] else None\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            query_time = (time.perf_counter() - query_start) * 1000\n",
        "            self.query_times.append(query_time)\n",
        "            \n",
        "            self.error_count = 0\n",
        "            return metrics\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            self.error_count += 1\n",
        "            self.dropped_samples += 1\n",
        "            if self.error_count <= 3:\n",
        "                print(f\"⚠️  nvidia-smi timeout (attempt {self.error_count})\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            self.error_count += 1\n",
        "            self.dropped_samples += 1\n",
        "            if self.error_count <= 3:\n",
        "                print(f\"⚠️  Error querying GPU metrics: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _monitor_loop(self):\n",
        "        \"\"\"Background monitoring loop with state tracking\"\"\"\n",
        "        interval_seconds = self.sampling_interval_ms / 1000.0\n",
        "        next_sample_time = time.perf_counter()\n",
        "        \n",
        "        while self.monitoring and self.error_count < self.max_errors:\n",
        "            current_time = time.perf_counter()\n",
        "            \n",
        "            if current_time >= next_sample_time:\n",
        "                metrics = self.get_gpu_metrics()\n",
        "                if metrics:\n",
        "                    self.timestamps.append(time.time())\n",
        "                    for key, value in metrics.items():\n",
        "                        self.metrics[key].append(value)\n",
        "                    \n",
        "                    # Capture current GPU state\n",
        "                    current_state = self.get_state()\n",
        "                    self.gpu_states.append(current_state)\n",
        "                    \n",
        "                    self.sample_count += 1\n",
        "                \n",
        "                next_sample_time += interval_seconds\n",
        "                \n",
        "                if self.adaptive_sampling and len(self.query_times) >= 10:\n",
        "                    avg_query_time = np.mean(list(self.query_times))\n",
        "                    if avg_query_time > (self.sampling_interval_ms * 0.8):\n",
        "                        if self.error_count == 0:\n",
        "                            print(f\"⚠️  Query time ({avg_query_time:.1f}ms) approaching sampling interval\")\n",
        "                        self.error_count = 1\n",
        "            else:\n",
        "                sleep_time = min(0.001, (next_sample_time - current_time) / 2)\n",
        "                if sleep_time > 0:\n",
        "                    time.sleep(sleep_time)\n",
        "\n",
        "    def start_monitoring(self):\n",
        "        \"\"\"Start monitoring in background thread\"\"\"\n",
        "        if self.monitoring:\n",
        "            print(\"⚠️  Monitoring already active\")\n",
        "            return\n",
        "            \n",
        "        self.monitoring = True\n",
        "        self.error_count = 0\n",
        "        self.sample_count = 0\n",
        "        self.dropped_samples = 0\n",
        "        \n",
        "        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
        "        self.monitor_thread.start()\n",
        "        print(f\"✅ Monitoring started at {self.sampling_interval_ms}ms intervals\")\n",
        "\n",
        "    def stop_monitoring(self):\n",
        "        \"\"\"Stop monitoring and return collected data\"\"\"\n",
        "        if not self.monitoring:\n",
        "            return None\n",
        "            \n",
        "        self.monitoring = False\n",
        "        if self.monitor_thread:\n",
        "            self.monitor_thread.join(timeout=2)\n",
        "\n",
        "        if self.sample_count > 0:\n",
        "            duration = self.timestamps[-1] - self.timestamps[0] if len(self.timestamps) > 1 else 0\n",
        "            actual_rate = self.sample_count / duration if duration > 0 else 0\n",
        "            expected_rate = 1000 / self.sampling_interval_ms\n",
        "            \n",
        "            print(f\"\\n📊 Monitoring Statistics:\")\n",
        "            print(f\"   Samples collected: {self.sample_count}\")\n",
        "            print(f\"   Dropped samples: {self.dropped_samples}\")\n",
        "            print(f\"   Duration: {duration:.2f}s\")\n",
        "            print(f\"   Actual rate: {actual_rate:.1f} samples/s\")\n",
        "            print(f\"   Efficiency: {(actual_rate/expected_rate*100):.1f}%\")\n",
        "\n",
        "        return self.get_dataframe()\n",
        "\n",
        "    def get_dataframe(self):\n",
        "        \"\"\"Convert collected metrics to DataFrame with state information\"\"\"\n",
        "        if not self.timestamps:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = pd.DataFrame(self.metrics)\n",
        "        df['timestamp'] = self.timestamps\n",
        "        df['time_ms'] = (df['timestamp'] - df['timestamp'].iloc[0]) * 1000\n",
        "        df['gpu_state'] = self.gpu_states  # Add GPU state column\n",
        "        \n",
        "        return df\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset all collected data\"\"\"\n",
        "        self.metrics = defaultdict(list)\n",
        "        self.timestamps = []\n",
        "        self.gpu_states = []\n",
        "        self.sample_count = 0\n",
        "        self.dropped_samples = 0\n",
        "        self.error_count = 0\n",
        "        self.query_times.clear()\n",
        "        self.current_state = \"idle\"\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL TRAINER WITH STATE TRACKING\n",
        "# ============================================================================\n",
        "\n",
        "class ModelTrainerWithStates:\n",
        "    \"\"\"Train models with detailed GPU state tracking\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet', device='cuda', sampling_interval_ms=100):\n",
        "        self.device = device\n",
        "        self.model_name = model_name\n",
        "        self.sampling_interval_ms = sampling_interval_ms\n",
        "        \n",
        "        # Initialize GPU monitor with state tracking\n",
        "        self.monitor = GPUMonitorWithStates(gpu_id=0, sampling_interval_ms=sampling_interval_ms)\n",
        "        \n",
        "        # Get GPU info\n",
        "        gpu_name, driver_version = self.monitor.get_gpu_info()\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"  🎮 GPU: {gpu_name}\")\n",
        "        print(f\"  🔧 Driver: {driver_version}\")\n",
        "        print(f\"  ⏱️  Sampling: {sampling_interval_ms}ms ({1000/sampling_interval_ms:.1f} samples/sec)\")\n",
        "        print(f\"  📊 State tracking: ENABLED\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        # Setup model\n",
        "        self.model = self._create_model()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "        # Storage for metrics\n",
        "        self.training_data = None\n",
        "        self.inference_data = None\n",
        "\n",
        "    def _create_model(self):\n",
        "        \"\"\"Create and prepare model\"\"\"\n",
        "        print(f\"📦 Loading {self.model_name} model...\")\n",
        "        \n",
        "        if self.model_name == 'resnet':\n",
        "            model = models.resnet50(pretrained=False)\n",
        "        elif self.model_name == 'vgg':\n",
        "            model = models.vgg16(pretrained=False)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model: {self.model_name}\")\n",
        "\n",
        "        model = model.to(self.device)\n",
        "        print(f\"✅ Model loaded on {self.device}\\n\")\n",
        "        return model\n",
        "\n",
        "    def train_epoch(self, dataloader, epoch):\n",
        "        \"\"\"Train for one epoch with state tracking\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "            # FORWARD PASS\n",
        "            self.monitor.set_state(f\"epoch_{epoch}_batch_{batch_idx+1}_forward_pass\")\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, target)\n",
        "            \n",
        "            # BACKWARD PASS\n",
        "            self.monitor.set_state(f\"epoch_{epoch}_batch_{batch_idx+1}_backward_pass\")\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # COMMUNICATION (simulated - in distributed training this would be gradient sync)\n",
        "            self.monitor.set_state(f\"epoch_{epoch}_batch_{batch_idx+1}_communication\")\n",
        "            torch.cuda.synchronize()  # Ensure all operations complete\n",
        "            time.sleep(0.01)  # Simulate brief communication delay\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 5 == 0:\n",
        "                print(f\"  Epoch {epoch} [{batch_idx+1}/{len(dataloader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def run_training(self, num_epochs=2, batch_size=48, num_batches=14):\n",
        "        \"\"\"Run training with state tracking\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"  🏋️  TRAINING PHASE WITH STATE TRACKING\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"  Epochs: {num_epochs}\")\n",
        "        print(f\"  Batch size: {batch_size}\")\n",
        "        print(f\"  Batches per epoch: {num_batches}\")\n",
        "        print(f\"  States tracked: initiation, forward_pass, backward_pass, communication\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        # Create dummy dataset\n",
        "        class DummyDataset(Dataset):\n",
        "            def __init__(self, size, num_classes=1000):\n",
        "                self.size = size\n",
        "                self.num_classes = num_classes\n",
        "\n",
        "            def __len__(self):\n",
        "                return self.size\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                return torch.randn(3, 224, 224), torch.randint(0, self.num_classes, (1,)).item()\n",
        "\n",
        "        dataset = DummyDataset(batch_size * num_batches)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # INITIATION STATE\n",
        "        self.monitor.set_state(\"training_initiation\")\n",
        "        self.monitor.start_monitoring()\n",
        "        time.sleep(0.5)  # Warmup period\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            avg_loss = self.train_epoch(dataloader, epoch)\n",
        "            print(f\"\\n  ✅ Epoch {epoch} complete - Avg Loss: {avg_loss:.4f}\\n\")\n",
        "\n",
        "        # Stop monitoring\n",
        "        self.monitor.set_state(\"training_complete\")\n",
        "        time.sleep(0.3)\n",
        "        self.training_data = self.monitor.stop_monitoring()\n",
        "\n",
        "        if self.training_data is not None and len(self.training_data) > 0:\n",
        "            state_counts = self.training_data['gpu_state'].value_counts()\n",
        "            print(f\"\\n✅ Training complete - {len(self.training_data)} samples collected\")\n",
        "            print(f\"\\n📊 State distribution:\")\n",
        "            for state, count in state_counts.head(10).items():\n",
        "                print(f\"   {state}: {count} samples\")\n",
        "        else:\n",
        "            print(\"\\n⚠️  Warning: No training data collected\")\n",
        "\n",
        "        cleanup_memory()\n",
        "\n",
        "    def run_inference(self, batch_size=64, num_batches=20, gamma_shape=2.0, gamma_scale=0.1):\n",
        "        \"\"\"\n",
        "        Run inference with state tracking.\n",
        "        States: processing_batch_N or waiting_for_queries\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"  🔍 INFERENCE PHASE WITH STATE TRACKING\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"  Number of user queries: {num_batches}\")\n",
        "        print(f\"  Batch size per query: {batch_size} images\")\n",
        "        print(f\"  Total images: {batch_size * num_batches}\")\n",
        "        print(f\"  Arrival pattern: Gamma distribution (shape={gamma_shape}, scale={gamma_scale}s)\")\n",
        "        print(f\"  States tracked: processing_batch_N, waiting_for_queries\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Generate inter-arrival times\n",
        "        np.random.seed(42)\n",
        "        inter_arrival_times = np.random.gamma(gamma_shape, gamma_scale, num_batches - 1)\n",
        "        \n",
        "        print(f\"📊 Query arrival statistics:\")\n",
        "        print(f\"   Mean inter-arrival time: {np.mean(inter_arrival_times)*1000:.1f}ms\")\n",
        "        print(f\"   Std inter-arrival time: {np.std(inter_arrival_times)*1000:.1f}ms\")\n",
        "        print(f\"   Expected total duration: {np.sum(inter_arrival_times):.2f}s\\n\")\n",
        "\n",
        "        # Reset and start monitoring\n",
        "        self.monitor.reset()\n",
        "        self.monitor.set_state(\"inference_initiation\")\n",
        "        self.monitor.start_monitoring()\n",
        "        time.sleep(0.2)\n",
        "\n",
        "        # Run inference with state tracking\n",
        "        query_timestamps = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(num_batches):\n",
        "                # PROCESSING STATE\n",
        "                self.monitor.set_state(f\"processing_batch_{i+1}\")\n",
        "                \n",
        "                query_start = time.time()\n",
        "                query_timestamps.append(query_start)\n",
        "                \n",
        "                dummy_input = torch.randn(batch_size, 3, 224, 224).to(self.device)\n",
        "                output = self.model(dummy_input)\n",
        "                torch.cuda.synchronize()\n",
        "                \n",
        "                query_duration = (time.time() - query_start) * 1000\n",
        "                print(f\"  Query {i+1}/{num_batches} complete (processing: {query_duration:.2f}ms)\")\n",
        "                \n",
        "                # WAITING STATE (if not last query)\n",
        "                if i < num_batches - 1:\n",
        "                    self.monitor.set_state(\"waiting_for_queries\")\n",
        "                    wait_time = inter_arrival_times[i]\n",
        "                    time.sleep(wait_time)\n",
        "\n",
        "        # Stop monitoring\n",
        "        self.monitor.set_state(\"inference_complete\")\n",
        "        time.sleep(0.2)\n",
        "        self.inference_data = self.monitor.stop_monitoring()\n",
        "\n",
        "        if self.inference_data is not None and len(self.inference_data) > 0:\n",
        "            state_counts = self.inference_data['gpu_state'].value_counts()\n",
        "            print(f\"\\n✅ Inference complete - {len(self.inference_data)} power samples collected\")\n",
        "            print(f\"\\n📊 State distribution:\")\n",
        "            for state, count in state_counts.items():\n",
        "                print(f\"   {state}: {count} samples\")\n",
        "            \n",
        "            # Calculate time in each state\n",
        "            processing_samples = sum(1 for s in self.inference_data['gpu_state'] if 'processing' in s)\n",
        "            waiting_samples = sum(1 for s in self.inference_data['gpu_state'] if 'waiting' in s)\n",
        "            total_samples = len(self.inference_data)\n",
        "            \n",
        "            print(f\"\\n⏱️  Time allocation:\")\n",
        "            print(f\"   Processing: {processing_samples/total_samples*100:.1f}% ({processing_samples} samples)\")\n",
        "            print(f\"   Waiting: {waiting_samples/total_samples*100:.1f}% ({waiting_samples} samples)\")\n",
        "        else:\n",
        "            print(\"\\n⚠️  Warning: No inference data collected\")\n",
        "\n",
        "        cleanup_memory()\n",
        "\n",
        "    def save_results(self, output_dir='/content'):\n",
        "        \"\"\"Save monitoring results with state metadata to CSV files\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"  💾 SAVING RESULTS WITH STATE METADATA\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        saved_files = []\n",
        "\n",
        "        if self.training_data is not None and len(self.training_data) > 0:\n",
        "            train_file = f'{output_dir}/training_metrics_with_states.csv'\n",
        "            self.training_data.to_csv(train_file, index=False)\n",
        "            print(f\"  ✅ Training metrics: {train_file}\")\n",
        "            print(f\"     Samples: {len(self.training_data)}\")\n",
        "            print(f\"     Columns: {', '.join(self.training_data.columns)}\")\n",
        "            saved_files.append(train_file)\n",
        "\n",
        "        if self.inference_data is not None and len(self.inference_data) > 0:\n",
        "            infer_file = f'{output_dir}/inference_metrics_with_states.csv'\n",
        "            self.inference_data.to_csv(infer_file, index=False)\n",
        "            print(f\"\\n  ✅ Inference metrics: {infer_file}\")\n",
        "            print(f\"     Samples: {len(self.inference_data)}\")\n",
        "            print(f\"     Columns: {', '.join(self.inference_data.columns)}\")\n",
        "            saved_files.append(infer_file)\n",
        "\n",
        "        print(f\"\\n{'='*70}\\n\")\n",
        "        return saved_files\n",
        "\n",
        "    def plot_results(self, output_dir='/content'):\n",
        "        \"\"\"Create visualization plots with state information\"\"\"\n",
        "        if self.training_data is None and self.inference_data is None:\n",
        "            print(\"⚠️  No data to plot\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "        fig.suptitle(f'GPU Metrics with State Tracking - {self.model_name.upper()} ({self.sampling_interval_ms}ms sampling)', \n",
        "                     fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Power Draw\n",
        "        ax = axes[0, 0]\n",
        "        if self.training_data is not None and 'power_draw_w' in self.training_data:\n",
        "            ax.plot(self.training_data['time_ms']/1000, self.training_data['power_draw_w'], \n",
        "                   label='Training', alpha=0.7, linewidth=1)\n",
        "        if self.inference_data is not None and 'power_draw_w' in self.inference_data:\n",
        "            ax.plot(self.inference_data['time_ms']/1000, self.inference_data['power_draw_w'], \n",
        "                   label='Inference', alpha=0.7, linewidth=1)\n",
        "        ax.set_xlabel('Time (seconds)')\n",
        "        ax.set_ylabel('Power Draw (W)')\n",
        "        ax.set_title('Power Consumption')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Temperature\n",
        "        ax = axes[0, 1]\n",
        "        if self.training_data is not None and 'temperature_c' in self.training_data:\n",
        "            ax.plot(self.training_data['time_ms']/1000, self.training_data['temperature_c'], \n",
        "                   label='Training', alpha=0.7, linewidth=1)\n",
        "        if self.inference_data is not None and 'temperature_c' in self.inference_data:\n",
        "            ax.plot(self.inference_data['time_ms']/1000, self.inference_data['temperature_c'], \n",
        "                   label='Inference', alpha=0.7, linewidth=1)\n",
        "        ax.set_xlabel('Time (seconds)')\n",
        "        ax.set_ylabel('Temperature (°C)')\n",
        "        ax.set_title('GPU Temperature')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # GPU Utilization\n",
        "        ax = axes[1, 0]\n",
        "        if self.training_data is not None and 'utilization_pct' in self.training_data:\n",
        "            ax.plot(self.training_data['time_ms']/1000, self.training_data['utilization_pct'], \n",
        "                   label='Training', alpha=0.7, linewidth=1)\n",
        "        if self.inference_data is not None and 'utilization_pct' in self.inference_data:\n",
        "            ax.plot(self.inference_data['time_ms']/1000, self.inference_data['utilization_pct'], \n",
        "                   label='Inference', alpha=0.7, linewidth=1)\n",
        "        ax.set_xlabel('Time (seconds)')\n",
        "        ax.set_ylabel('Utilization (%)')\n",
        "        ax.set_title('GPU Utilization')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Memory Usage\n",
        "        ax = axes[1, 1]\n",
        "        if self.training_data is not None and 'memory_used_mb' in self.training_data:\n",
        "            ax.plot(self.training_data['time_ms']/1000, self.training_data['memory_used_mb']/1024, \n",
        "                   label='Training', alpha=0.7, linewidth=1)\n",
        "        if self.inference_data is not None and 'memory_used_mb' in self.inference_data:\n",
        "            ax.plot(self.inference_data['time_ms']/1000, self.inference_data['memory_used_mb']/1024, \n",
        "                   label='Inference', alpha=0.7, linewidth=1)\n",
        "        ax.set_xlabel('Time (seconds)')\n",
        "        ax.set_ylabel('Memory Used (GB)')\n",
        "        ax.set_title('GPU Memory Usage')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        plot_file = f'{output_dir}/gpu_metrics_with_states.png'\n",
        "        plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
        "        print(f\"\\n📊 Plot saved: {plot_file}\")\n",
        "        plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  GPU POWER MONITORING WITH STATE TRACKING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Configuration\n",
        "    SAMPLING_INTERVAL_MS = 50  # 50ms = 20 samples per second\n",
        "    \n",
        "    MODEL_NAME = 'resnet'\n",
        "    NUM_EPOCHS = 2\n",
        "    TRAIN_BATCH_SIZE = 48\n",
        "    TRAIN_BATCHES = 14\n",
        "    \n",
        "    INFER_BATCH_SIZE = 64\n",
        "    INFER_BATCHES = 20  # Number of user queries\n",
        "    GAMMA_SHAPE = 2.0\n",
        "    GAMMA_SCALE = 0.1  # Mean inter-arrival = 0.2s\n",
        "\n",
        "    try:\n",
        "        # Check GPU availability\n",
        "        if not torch.cuda.is_available():\n",
        "            raise RuntimeError(\"No GPU available! Enable GPU runtime in Colab.\")\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = ModelTrainerWithStates(\n",
        "            model_name=MODEL_NAME,\n",
        "            device='cuda',\n",
        "            sampling_interval_ms=SAMPLING_INTERVAL_MS\n",
        "        )\n",
        "\n",
        "        # Run training with state tracking\n",
        "        trainer.run_training(\n",
        "            num_epochs=NUM_EPOCHS,\n",
        "            batch_size=TRAIN_BATCH_SIZE,\n",
        "            num_batches=TRAIN_BATCHES\n",
        "        )\n",
        "\n",
        "        # Run inference with state tracking\n",
        "        trainer.run_inference(\n",
        "            batch_size=INFER_BATCH_SIZE,\n",
        "            num_batches=INFER_BATCHES,\n",
        "            gamma_shape=GAMMA_SHAPE,\n",
        "            gamma_scale=GAMMA_SCALE\n",
        "        )\n",
        "\n",
        "        # Save results\n",
        "        saved_files = trainer.save_results()\n",
        "\n",
        "        # Create plots\n",
        "        trainer.plot_results()\n",
        "\n",
        "        # Summary\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"  ✅ EXECUTION COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"  Model              : {MODEL_NAME}\")\n",
        "        print(f\"  Sampling rate      : {SAMPLING_INTERVAL_MS}ms\")\n",
        "        print(f\"  State tracking     : ENABLED\")\n",
        "        print(f\"  Training states    : initiation, forward_pass, backward_pass, communication\")\n",
        "        print(f\"  Inference states   : processing_batch_N, waiting_for_queries\")\n",
        "        if trainer.training_data is not None:\n",
        "            print(f\"  Training samples   : {len(trainer.training_data)}\")\n",
        "        if trainer.inference_data is not None:\n",
        "            print(f\"  Inference samples  : {len(trainer.inference_data)}\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        cleanup_memory()\n"
      ],
      "metadata": {
        "id": "main_code_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download results\n",
        "!zip -r results_with_states.zip /content/*.csv /content/*.png\n",
        "\n",
        "from google.colab import files\n",
        "files.download('results_with_states.zip')"
      ],
      "metadata": {
        "id": "download_cell"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
